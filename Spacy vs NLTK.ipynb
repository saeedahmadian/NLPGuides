{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This jupyter notebook contains two different libraries for Spacy and NLTK and #tensorflow.keras.preprocessing.text and #torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First lets recap regex library and some of the important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Major RE functions\n",
    "\n",
    "re.findall - Module is used to search for “all” occurrences that match a given pattern.\n",
    "\n",
    "\n",
    "re.sub - Substitute the matched RE patter with given text\n",
    "\n",
    "\n",
    "re.match - The match function is used to match the RE pattern to string with optional flags\n",
    "\n",
    "\n",
    "re.search - This method takes a regular expression pattern and a string and searches for that pattern with the string.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Original Article: In computer science, lexical analysis, lexing or \n",
    "    tokenization is the process of converting a sequence of characters \n",
    "    (such as in a computer program or web page) into a sequence of tokens \n",
    "    (strings with an assigned and thus identified meaning). \n",
    "    A program that performs lexical analysis may be termed a lexer, \n",
    "    tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer.\n",
    "    A lexer is generally combined with a parser, which together analyze the syntax of programming languages,\n",
    "    web pages, and so forth.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':', '.', '.', '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'([:?\\.]+)',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_md\n",
    "nlp= en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Stop words in Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words= nlp.Defaults.stop_words\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Sentence and Word Tokenize with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence : 0\n",
      "Original Article:\n",
      "sentence : 1\n",
      "In computer science, lexical analysis, lexing or \n",
      "    tokenization is the process of converting a sequence of characters \n",
      "    (such as in a computer program or web page) into a sequence of tokens \n",
      "    \n",
      "sentence : 2\n",
      "(strings with an assigned and thus identified meaning). \n",
      "    \n",
      "sentence : 3\n",
      "A program that performs lexical analysis may be termed a lexer, \n",
      "    tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer.\n",
      "    \n",
      "sentence : 4\n",
      "A lexer is generally combined with a parser, which together analyze the syntax of programming languages,\n",
      "    web pages, and so forth.\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(text)\n",
    "sent_generator= doc.sents\n",
    "bagOfwords=[]\n",
    "for i,sent in enumerate(sent_generator):\n",
    "    # sent now is a span\n",
    "    print('sentence : {}'.format(i))\n",
    "    print(sent.text)\n",
    "    for word in sent:\n",
    "        bagOfwords.append(word.text)\n",
    "        # word now is a token\n",
    "        # word.text create a string\n",
    "#         print(\"the token is : {}\".format(word))\n",
    "#         print(\"the string of token is : {}\".format(word.text))\n",
    "#         print(\"the lemma of token is : {} \".format(word.lemma_))\n",
    "#         print(\"the position of token is : {} \".format(word.pos_))\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Lemmatization and Stemming in SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['have', 'have', 'have']\n",
      "['have', 'have', 'have']\n",
      "['have', 'ha', 'had']\n"
     ]
    }
   ],
   "source": [
    "### Note spaCy do not have stemming. Due to the reason that Lemmatization is seen as more informative than stemming.\n",
    "test=\"have has had\"\n",
    "doc= nlp(test)\n",
    "print([token.lemma_ for token in doc])\n",
    "# vs how we nltk\n",
    "print([nltk.stem.WordNetLemmatizer().lemmatize(token,pos='v') for token in nltk.tokenize.word_tokenize(test)])\n",
    "print([nltk.stem.PorterStemmer().stem(token) for token in nltk.tokenize.word_tokenize(test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Stop words in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words= stopwords.words('english')\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Tokenize with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "## or we can also directly call sent_tokenize and word_tokenize as below\n",
    "\n",
    "# from nltk import sent_tokenize\n",
    "# from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence : 0\n",
      "Original Article: In computer science, lexical analysis, lexing or \n",
      "    tokenization is the process of converting a sequence of characters \n",
      "    (such as in a computer program or web page) into a sequence of tokens \n",
      "    (strings with an assigned and thus identified meaning).\n",
      "sentence : 1\n",
      "A program that performs lexical analysis may be termed a lexer, \n",
      "    tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer.\n",
      "sentence : 2\n",
      "A lexer is generally combined with a parser, which together analyze the syntax of programming languages,\n",
      "    web pages, and so forth.\n"
     ]
    }
   ],
   "source": [
    "sentences= sent_tokenize(text)\n",
    "bagOfwords=[]\n",
    "for i,sent in enumerate(sentences):\n",
    "    print('sentence : {}'.format(i))\n",
    "    print(sent)\n",
    "    for word in word_tokenize(sent):\n",
    "        bagOfwords.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Lemmatization & Stemming in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\saeed.ahmadian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have', 'had', 'ha']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming\n",
    "######################\n",
    "text=\"List listed lists listing listings\"\n",
    "text=\"have had has\"\n",
    "words=text.lower().split(' ')\n",
    "# or\n",
    "words= word_tokenize(text)\n",
    "\n",
    "stemmer= PorterStemmer()\n",
    "## Stem words\n",
    "stemmed_words=[stemmer.stem(word) for word in words]\n",
    "# or\n",
    "[PorterStemmer().stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have', 'have', 'have']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### Lemmatize\n",
    "lemma= WordNetLemmatizer()\n",
    "[lemma.lemmatize(word,pos='v') for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've ever worked on a project for deep learning for NLP, you'll know how painful and tedious all the preprocessing is. Before you start training your model, you have to:\n",
    "\n",
    "    1) Read the data from disk\n",
    "\n",
    "    2) Tokenize the text\n",
    "\n",
    "    3) Create a mapping from word to a unique integer\n",
    "\n",
    "    4) Convert the text into lists of integers\n",
    "\n",
    "    5) Load the data in whatever format your deep learning framework requires\n",
    "\n",
    "    6) Pad the text so that all the sequences are the same length, so you can process them in batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Read the data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train=pd.read_csv('train/train.csv',header='infer')\n",
    "dd=df_train.head(100)\n",
    "dd.to_csv('dd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['comment_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Explanation Why the edits made under my username Hardcore Metallica Fan were reverted?,\n",
       " They were not vandalisms, just closure on some GAs after I voted at New York Dolls FAC.,\n",
       " And please do not remove the template from the talk page since I'm retired now.89.205.38.27]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent=\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted??## They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\"\n",
    "sent= re.sub(r'[^a-zA-Z0-9!?,\\.\\']',' ',sent)\n",
    "sent=re.sub(r'([\\?!,\\.]+)',lambda m: m.group(1)[0],sent)\n",
    "sent=re.sub(r'[ ]+',' ',sent)\n",
    "sent= re.sub(r'n\\'t',r' not',sent)\n",
    "# sent= re.sub('\\'',r' ddd',sent)\n",
    "list(nlp(sent).sents)\n",
    "# stop_words=nltk.corpus.stopwords.words('english')\n",
    "# stop_words.remove('not')\n",
    "# doc= nlp(sent)\n",
    "# [token.lemma_ for token in nltk.tokenize.word_tokenize()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Use any of the spacy or nltk methods in any level\"\n",
    "\"here I use nltk\"\n",
    "\n",
    "stop_words=nltk.corpus.stopwords.words('english')\n",
    "stop_words.remove('not')\n",
    "def CustomTokenizerSpacy(sentence):\n",
    "#     sentence= re.sub('n\\'t',r' not',sentence)\n",
    "#     sentence=re.sub(r'[^a-zA-Z0-9!?,\\.]',' ',sentence)\n",
    "#     sentence= re.sub(r'([\\?!,\\.]+)',lambda m: m.group(1)[0],sentence)\n",
    "#     sentence=re.sub(r'[ ]+',' ',sentence)\n",
    "    return [token.lemma_ for token in nlp.tokenizer(sentence)]# if token.lemma_ not in nlp.Defaults.stop_words\n",
    "\n",
    "def CustomTokenizerNltk(sentence):\n",
    "    sentence=  re.sub(r'[^a-zA-Z0-9!?,\\.\\']',' ',sentence)\n",
    "    sentence= re.sub(r'([\\?!,\\.]+)',lambda m: m.group(1)[0],sentence)\n",
    "    sentence=re.sub(r'[ ]+',' ',sentence)\n",
    "    sentence= re.sub('n\\'t',r' not',sentence)\n",
    "#     stemmer=nltk.stem.WordNetLemmatizer()\n",
    "    stemmer= nltk.stem.PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in nltk.tokenize.word_tokenize(sentence) if token not in stop_words]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train['comment_text'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_nltk=CustomTokenizerNltk(df_train['comment_text'][1])\n",
    "tokens_spacy=CustomTokenizerSpacy(df_train['comment_text'][1])\n",
    "mismatch=[]\n",
    "# for i in range(len(tokens_nltk)):\n",
    "#     if tokens_nltk[i]!=tokens_spacy[i]:\n",
    "#         mismatch.append((tokens_nltk[i],tokens_spacy[i]))\n",
    "# print(mismatch)\n",
    "type(tokens_spacy[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Create a mapping from word to a unique integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "comment_text= Field(sequential=True,use_vocab=True,\n",
    "                    lower=True,batch_first=True\n",
    "                   ,tokenize=CustomTokenizerSpacy\n",
    "                   ) # we don use use_vocab=True because we are using our own toeknizer\n",
    "toxic= Field(sequential=False,use_vocab=False)\n",
    "severe_toxic= Field(sequential=False,use_vocab=False)\n",
    "obscene= Field(sequential=False,use_vocab=False)\n",
    "threat= Field(sequential=False,use_vocab=False)\n",
    "insult= Field(sequential=False,use_vocab=False)\n",
    "identity_hate= Field(sequential=False,use_vocab=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.7.0'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchtext\n",
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "train_datafields=[('id',None),('comment_text',comment_text),('toxic',toxic),('severe_toxic',severe_toxic),\n",
    "                 ('obscene',obscene),('threat',threat),('insult',insult),('identity_hate',identity_hate)]\n",
    "train_set= TabularDataset(path='train/train.csv',format='csv',fields=train_datafields,skip_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210590"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_text.build_vocab(train_set)\n",
    "len(comment_text.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16012"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_text.vocab.stoi['went']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'went'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_text.vocab.itos[664]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using W2vec also we can have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_text.build_vocab(train_set,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
